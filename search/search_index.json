{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The ML.ENERGY Data &amp; Toolkit","text":"<p>ML.ENERGY publishes open-source datasets. To aid in working with these datasets, we also provide a Python toolkit: <code>mlenergy-data</code>.</p> <p>We currently have The ML.ENERGY Benchmark v3.0 dataset, which includes LLM and diffusion inference runs on NVIDIA H100 and B200 GPUs. Actual data are stored in Hugging Face Hub: <code>ml-energy/benchmark-v3</code>.</p>"},{"location":"#what-the-toolkit-does","title":"What the Toolkit Does","text":"<ul> <li>Load and filter benchmark runs with typed, immutable collection classes (<code>LLMRuns</code>, <code>DiffusionRuns</code>).</li> <li>Extract bulk per-request detailed data (e.g., power timelines, ITL samples, output lengths) as DataFrames.</li> <li>Fit models: logistic power/latency curves and ITL latency distributions.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install mlenergy-data\n</code></pre>"},{"location":"#dataset-access","title":"Dataset Access","text":"<p>The benchmark dataset (<code>ml-energy/benchmark-v3</code>) is gated on Hugging Face Hub. Before using the toolkit to load data from HF, you need to:</p> <ol> <li>Visit the dataset page and request access (granted automatically).</li> <li>Set the <code>HF_TOKEN</code> environment variable to a Hugging Face access token.</li> </ol>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from mlenergy_data.records import LLMRuns\n\nruns = LLMRuns.from_hf()\n\n# Find the minimum-energy model on GPQA\ngpqa_runs = runs.task(\"gpqa\")\nbest = min(gpqa_runs, key=lambda r: r.energy_per_token_joules)\nprint(f\"{best.nickname}: {best.energy_per_token_joules:.3f} J/tok on {best.gpu_model}\")\n\nenergies = [r.energy_per_token_joules for r in runs]\n</code></pre> <p>Filter, group, and compare across GPU generations and model architectures:</p> <pre><code># Compare GPU generations: best energy efficiency per model on GPQA\nfor gpu, group in runs.task(\"gpqa\").group_by(\"gpu_model\").items():\n    best = min(group, key=lambda r: r.energy_per_token_joules)\n    print(f\"{gpu}: {best.nickname} @ {best.energy_per_token_joules:.3f} J/tok, \"\n          f\"{best.output_throughput_tokens_per_sec:.0f} tok/s\")\n\n# MoE, Dense, Hybrid: who's more energy-efficient?\nfor arch, group in runs.task(\"gpqa\").gpu_model(\"B200\").group_by(\"architecture\").items():\n    best = min(group, key=lambda r: r.energy_per_token_joules)\n    print(f\"{arch}: {best.nickname} @ {best.energy_per_token_joules:.3f} J/tok\")\n</code></pre>"},{"location":"#who-uses-it","title":"Who Uses It","text":"<ul> <li>The ML.ENERGY Leaderboard v3.0: Benchmark results are loaded and compiled into the leaderboard web app data format.</li> <li>OpenG2G: Datacenter-grid coordination simulation framework; loads benchmark data and fits models.</li> <li>The ML.ENERGY blog: Analysis scripts for blog posts.</li> </ul> <p>See the Guide page for more details, together with a progressive walkthrough.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Guide: Progressive walkthrough from loading data to fitting models.</li> <li>API Reference: Auto-generated from docstrings.</li> </ul>"},{"location":"#citation","title":"Citation","text":"<pre><code>@inproceedings{mlenergy-neuripsdb25,\n    title={The {ML.ENERGY Benchmark}: Toward Automated Inference Energy Measurement and Optimization},\n    author={Jae-Won Chung and Jeff J. Ma and Ruofan Wu and Jiachen Liu and Oh Jun Kweon and Yuxuan Xia and Zhiyu Wu and Mosharaf Chowdhury},\n    year={2025},\n    booktitle={NeurIPS Datasets and Benchmarks},\n}\n</code></pre>"},{"location":"guide/","title":"Toolkit Guide","text":""},{"location":"guide/#real-world-examples","title":"Real-World Examples","text":"<p>For full working examples of the toolkit in production, see:</p> Project Description Script The ML.ENERGY Leaderboard Builds the leaderboard JSON data from benchmark runs Link The ML.ENERGY Blog Analysis for the blog post on the V3 benchmark results Link OpenG2G Simulation Power traces and models for datacenter-grid simulation Link"},{"location":"guide/#dataset-access","title":"Dataset Access","text":"<p>The benchmark dataset (<code>ml-energy/benchmark-v3</code>) is gated on Hugging Face Hub. Before loading data with <code>from_hf()</code>, you need to:</p> <ol> <li>Visit the dataset page and request access (granted automatically).</li> <li>Set the <code>HF_TOKEN</code> environment variable to a Hugging Face access token.</li> </ol>"},{"location":"guide/#loading-benchmark-runs","title":"Loading Benchmark Runs","text":"<p><code>LLMRuns</code> and <code>DiffusionRuns</code> are typed, immutable collections. Each run is a frozen dataclass (<code>LLMRun</code> / <code>DiffusionRun</code>) with IDE autocomplete and type checking.</p> <pre><code>from mlenergy_data.records import LLMRuns, DiffusionRuns\n\n# Load from Hugging Face Hub\nruns = LLMRuns.from_hf()\n\n# Diffusion runs\ndiff = DiffusionRuns.from_hf()\n</code></pre> <p>Or load from a local compiled data directory:</p> <pre><code>root = \"/path/to/compiled/data\"\nruns = LLMRuns.from_directory(root)\nruns = LLMRuns.from_directory(root, stable_only=False)\ndiff = DiffusionRuns.from_directory(root)\n</code></pre> <p>Note</p> <p>A \"compiled data directory\" is one built by <code>data_publishing/build_hf_data.py</code> (or downloaded from HF Hub). It contains parquet summary files under <code>runs/</code>, raw result files under <code>llm/</code> and <code>diffusion/</code>, and benchmark config files under <code>configs/</code>.</p>"},{"location":"guide/#filtering","title":"Filtering","text":"<p>All filter methods return a new collection; chain freely:</p> <pre><code># Single filter\ngpqa = runs.task(\"gpqa\")\nh100 = runs.gpu_model(\"H100\")\nfp8 = runs.precision(\"fp8\")\n\n# Chained filters\nbest_candidates = runs.task(\"gpqa\").gpu_model(\"B200\").precision(\"fp8\")\n\n# Multiple values (OR within a filter)\nchat_or_gpqa = runs.task(\"gpqa\", \"lm-arena-chat\")\n\n# By nickname\ndeepseek = runs.nickname(\"DeepSeek R1\")\n\n# Architecture (LLM only)\nmoe_models = runs.architecture(\"MoE\")\n\n# Batch size: exact values or range\nbatch_128 = runs.max_num_seqs(128)\nlarge_batch = runs.max_num_seqs(min=64)\nmid_batch = runs.max_num_seqs(min=16, max=128)\n\n# GPU count: exact or range\nsingle_gpu = runs.num_gpus(1)\nmulti_gpu = runs.num_gpus(min=2)\n\n# Arbitrary predicate\nbig_models = runs.where(lambda r: r.total_params_billions &gt; 70)\n</code></pre>"},{"location":"guide/#data-access","title":"Data Access","text":"<p>Iterate the collection to get individual typed records:</p> <pre><code>for r in runs.task(\"gpqa\"):\n    print(r.energy_per_token_joules, r.nickname)\n\nbest = min(runs.task(\"gpqa\"), key=lambda r: r.energy_per_token_joules)\nprint(f\"{best.nickname}: {best.energy_per_token_joules:.3f} J/tok\")\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\nplt.scatter(\n    [r.max_num_seqs for r in runs],\n    [r.energy_per_token_joules for r in runs],\n)\nplt.xlabel(\"Batch size\")\nplt.ylabel(\"Energy per token (J)\")\n</code></pre> <p>Indexing and concatenation:</p> <pre><code>first_run = runs[0]\n\n# Concatenate collections\nh100 = runs.gpu_model(\"H100\")\nb200 = runs.gpu_model(\"B200\")\ncombined = h100 + b200\n</code></pre>"},{"location":"guide/#grouping","title":"Grouping","text":"<pre><code># Group by task\nfor task, group in runs.group_by(\"task\").items():\n    print(f\"{task}: {len(group)} runs\")\n\n# Group by multiple fields\nfor (model, batch), g in runs.group_by(\"model_id\", \"max_num_seqs\").items():\n    best = min(g, key=lambda r: r.energy_per_token_joules)\n    print(f\"{model} @ batch={batch}: {best.energy_per_token_joules:.3f} J/tok\")\n</code></pre>"},{"location":"guide/#analysis-patterns","title":"Analysis Patterns","text":"<p>Python is the analysis layer. No special helper functions needed:</p> <pre><code># Compare GPU generations on a task\nfor gpu, group in runs.task(\"lm-arena-chat\").group_by(\"gpu_model\").items():\n    best = min(group, key=lambda r: r.output_throughput_tokens_per_sec)\n    print(f\"{gpu}: {best.nickname} @ {best.output_throughput_tokens_per_sec:.0f} tok/s\")\n\n# Comparing GPUs for a specific model\nllama70b = runs.model_id(\"meta-llama/Llama-3.1-70B-Instruct\")\nfor gpu, g in llama70b.group_by(\"gpu_model\").items():\n    plt.scatter(\n        [r.max_num_seqs for r in g],\n        [r.energy_per_token_joules for r in g],\n        label=gpu,\n    )\nplt.legend()\n</code></pre>"},{"location":"guide/#bulk-raw-data","title":"Bulk Raw Data","text":"<p>By default, only the compact Parquet files are downloaded, but for some analyses, you need the raw results file.</p> <p>These methods return pandas DataFrames for numerical analysis. When loaded from HF Hub (<code>from_hf()</code>), the library automatically downloads only the raw files needed for the current collection. The download scope is determined by your filters (e.g., <code>task()</code>, <code>gpu_model()</code>). HF Hub caches files locally, so repeated calls are instant.</p> <p>To eagerly download all raw files upfront, use <code>download_raw_files()</code>:</p> <pre><code># Eagerly download all raw files for a filtered collection\nruns = LLMRuns.from_hf().task(\"lm-arena-chat\").gpu_model(\"H100\").download_raw_files()\npower_tl = runs.timelines(metric=\"power.device_instant\")  # no download delay\n</code></pre> <pre><code># Power timelines (long-form)\npower_tl = runs.timelines(metric=\"power.device_instant\")\n\n# Temperature timelines\ntemp_tl = runs.timelines(metric=\"temperature\")\n\n# Output lengths\nout_df = runs.output_lengths()\n\n# Full DataFrame (one row per run, all fields as columns)\ndf = runs.to_dataframe()\n</code></pre> <p>Per-record methods give you direct access to raw data without needing a join key:</p> <pre><code>run = runs[0]\n\n# Parsed results.json (downloaded lazily from HF Hub, then cached)\ndata = run.read_results_json()\n\n# Per-record data extraction\nlengths = run.output_lengths()\nitls = run.inter_token_latencies()\ntl = run.timelines(metric=\"power.device_instant\")\n</code></pre>"},{"location":"guide/#diffusion-runs","title":"Diffusion Runs","text":"<p><code>DiffusionRuns</code> follows the same patterns:</p> <pre><code>from mlenergy_data.records import DiffusionRuns\n\ndiff = DiffusionRuns.from_hf()\nt2i = diff.task(\"text-to-image\")\nbest = min(t2i, key=lambda r: r.energy_per_generation_joules)\nprint(f\"{best.nickname}: {best.energy_per_generation_joules:.3f} J/image\")\n\n# Task field and convenience properties\nr = diff[0]\nr.task               # \"text-to-image\" or \"text-to-video\"\nr.is_text_to_image   # True for text-to-image tasks\nr.is_text_to_video   # True for text-to-video tasks\n\n# Available filters: task(), model(), gpu(), nickname(), batch(),\n# num_gpus(), precision(), where()\n</code></pre>"},{"location":"guide/#model-fitting","title":"Model Fitting","text":""},{"location":"guide/#logistic-curves","title":"Logistic Curves","text":"<p><code>LogisticModel</code> models a four-parameter logistic <code>y = b0 + L * sigmoid(k * (x - x0))</code> where <code>x = log2(batch_size)</code>:</p> <pre><code>import numpy as np\nfrom mlenergy_data.modeling import LogisticModel\n\n# Fit from data\nx = np.log2([8, 16, 32, 64, 128, 256])\ny_power = np.array([200, 250, 320, 400, 480, 530])\nfit = LogisticModel.fit(x, y_power)\n\n# Evaluate at a specific batch size\npredicted = fit.eval(batch=128)\n\n# Serialize / deserialize\nd = fit.to_dict()  # {\"L\": ..., \"x0\": ..., \"k\": ..., \"b0\": ...}\nfit2 = LogisticModel.from_dict(d)\n</code></pre>"},{"location":"guide/#itl-latency-distributions","title":"ITL Latency Distributions","text":"<p><code>ITLMixtureModel</code> fits a two-component lognormal mixture for inter-token latency:</p> <pre><code>from mlenergy_data.modeling import ITLMixtureModel\n\n# Fit from raw ITL samples (seconds)\nmodel = ITLMixtureModel.fit(itl_samples_s, max_samples=2048, seed=0)\n\n# Analytical mean and variance\nmean, var = model.mean_var()\n\n# Simulate average ITL across replicas\nrng = np.random.default_rng(0)\navg_itl = model.sample_avg(n_replicas=180, rng=rng)\n\n# Serialize / deserialize\nd = model.to_dict()\nmodel2 = ITLMixtureModel.from_dict(d)\n</code></pre>"},{"location":"api/modeling/","title":"Modeling API","text":"<p>Logistic curve fitting and ITL latency distribution modeling.</p>"},{"location":"api/modeling/#logisticmodel","title":"LogisticModel","text":""},{"location":"api/modeling/#mlenergy_data.modeling.logistic.LogisticModel","title":"<code>mlenergy_data.modeling.logistic.LogisticModel</code>  <code>dataclass</code>","text":"<p>Four-parameter logistic: <code>y = b0 + L * sigmoid(k * (x - x0))</code>.</p> <p><code>x</code> is typically <code>log2(batch_size)</code>.</p> <p>Attributes:</p> Name Type Description <code>L</code> <code>float</code> <p>Amplitude (upper asymptote minus lower asymptote).</p> <code>x0</code> <code>float</code> <p>Midpoint of the sigmoid on the x-axis.</p> <code>k</code> <code>float</code> <p>Steepness of the sigmoid curve.</p> <code>b0</code> <code>float</code> <p>Baseline offset (lower asymptote).</p>"},{"location":"api/modeling/#mlenergy_data.modeling.logistic.LogisticModel.eval_x","title":"<code>eval_x(x)</code>","text":"<p>Evaluate at continuous x (typically log2(batch_size)).</p>"},{"location":"api/modeling/#mlenergy_data.modeling.logistic.LogisticModel.deriv_wrt_x","title":"<code>deriv_wrt_x(x)</code>","text":"<p>Derivative dy/dx at continuous x.</p>"},{"location":"api/modeling/#mlenergy_data.modeling.logistic.LogisticModel.eval","title":"<code>eval(batch)</code>","text":"<p>Evaluate at an integer batch size (converted to log2).</p>"},{"location":"api/modeling/#mlenergy_data.modeling.logistic.LogisticModel.fit","title":"<code>fit(x, y)</code>  <code>classmethod</code>","text":"<p>Fit four-parameter logistic using coarse grid search + least-squares.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Independent variable (typically log2(batch_size)).</p> required <code>y</code> <code>ndarray</code> <p>Dependent variable (e.g., power, latency, throughput).</p> required <p>Returns:</p> Type Description <code>LogisticModel</code> <p>Fitted LogisticModel instance.</p>"},{"location":"api/modeling/#mlenergy_data.modeling.logistic.LogisticModel.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize parameters to a dict.</p>"},{"location":"api/modeling/#mlenergy_data.modeling.logistic.LogisticModel.from_dict","title":"<code>from_dict(d)</code>  <code>classmethod</code>","text":"<p>Deserialize parameters from a dict (or dict-like, e.g. pandas Series).</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict[str, Any] | Any</code> <p>Dict with keys <code>L</code>, <code>x0</code>, <code>k</code>, <code>b0</code>.</p> required"},{"location":"api/modeling/#itlmixturemodel","title":"ITLMixtureModel","text":""},{"location":"api/modeling/#mlenergy_data.modeling.latency.ITLMixtureModel","title":"<code>mlenergy_data.modeling.latency.ITLMixtureModel</code>  <code>dataclass</code>","text":"<p>Two-component ITL mixture where each component is log-normal plus offset.</p> <p>Attributes:</p> Name Type Description <code>loc</code> <code>float</code> <p>Location shift applied to all samples.</p> <code>pi_steady</code> <code>float</code> <p>Mixture weight for the steady (low-latency) component.</p> <code>sigma_steady</code> <code>float</code> <p>Log-normal sigma for the steady component.</p> <code>scale_steady</code> <code>float</code> <p>Log-normal scale for the steady component.</p> <code>pi_stall</code> <code>float</code> <p>Mixture weight for the stall (high-latency) component.</p> <code>sigma_stall</code> <code>float</code> <p>Log-normal sigma for the stall component.</p> <code>scale_stall</code> <code>float</code> <p>Log-normal scale for the stall component.</p>"},{"location":"api/modeling/#mlenergy_data.modeling.latency.ITLMixtureModel.mean_var","title":"<code>mean_var()</code>","text":"<p>Compute analytical mean and variance of the two-component mixture.</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>Tuple of (mean, variance) in seconds.</p>"},{"location":"api/modeling/#mlenergy_data.modeling.latency.ITLMixtureModel.sample_one","title":"<code>sample_one(rng)</code>","text":"<p>Draw a single ITL sample from the mixture.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>NumPy random generator.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Sampled ITL value in seconds.</p>"},{"location":"api/modeling/#mlenergy_data.modeling.latency.ITLMixtureModel.sample_avg","title":"<code>sample_avg(*, n_replicas, rng, exact_threshold=30)</code>","text":"<p>Sample the average ITL across n_replicas replicas.</p> <p>For n &lt;= exact_threshold: draw n individual samples and average. For n &gt; exact_threshold: use CLT approximation (normal from mean/var).</p> <p>Parameters:</p> Name Type Description Default <code>n_replicas</code> <code>int</code> <p>Number of replicas to average over.</p> required <code>rng</code> <code>Generator</code> <p>NumPy random generator.</p> required <code>exact_threshold</code> <code>int</code> <p>Below this count, use exact sampling.</p> <code>30</code> <p>Returns:</p> Type Description <code>float</code> <p>Average ITL in seconds.  NaN if n_replicas &lt;= 0.</p>"},{"location":"api/modeling/#mlenergy_data.modeling.latency.ITLMixtureModel.fit","title":"<code>fit(samples_s, *, max_samples=None, seed=0)</code>  <code>classmethod</code>","text":"<p>Fit a two-component lognormal mixture from ITL samples.</p> <p>Parameters:</p> Name Type Description Default <code>samples_s</code> <code>ndarray</code> <p>Raw ITL samples in seconds.</p> required <code>max_samples</code> <code>int | None</code> <p>If set, randomly subsample to this many values before fitting.</p> <code>None</code> <code>seed</code> <code>int</code> <p>RNG seed for subsampling.</p> <code>0</code> <p>Returns:</p> Type Description <code>ITLMixtureModel</code> <p>Fitted ITLMixtureModel instance.</p>"},{"location":"api/modeling/#mlenergy_data.modeling.latency.ITLMixtureModel.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize parameters to a dict.</p>"},{"location":"api/modeling/#mlenergy_data.modeling.latency.ITLMixtureModel.from_dict","title":"<code>from_dict(d)</code>  <code>classmethod</code>","text":"<p>Deserialize parameters from a dict (or dict-like, e.g. pandas Series).</p> <p>The dict keys may optionally use the <code>itl_mix_</code> prefix (e.g., from CSV files produced by the build pipeline).</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict[str, Any] | Any</code> <p>Dict with keys for all 7 mixture parameters.</p> required"},{"location":"api/records/","title":"Records API","text":"<p>Typed collection classes for benchmark runs.</p>"},{"location":"api/records/#llmrun","title":"LLMRun","text":""},{"location":"api/records/#mlenergy_data.records.runs.LLMRun","title":"<code>mlenergy_data.records.runs.LLMRun</code>  <code>dataclass</code>","text":"<p>A single LLM benchmark run.</p> <p>Attributes:</p> Name Type Description <code>domain</code> <code>str</code> <p>Always <code>\"llm\"</code>.</p> <code>task</code> <code>str</code> <p>Benchmark task (e.g. <code>\"gpqa\"</code>, <code>\"lm-arena-chat\"</code>).</p> <code>model_id</code> <code>str</code> <p>Full HF model identifier (e.g. <code>\"meta-llama/Llama-3.1-8B-Instruct\"</code>).</p> <code>nickname</code> <code>str</code> <p>Human-friendly display name from <code>model_info.json</code>.</p> <code>architecture</code> <code>str</code> <p>Model architecture (<code>\"Dense\"</code> or <code>\"MoE\"</code>).</p> <code>total_params_billions</code> <code>float</code> <p>Total parameter count in billions.</p> <code>activated_params_billions</code> <code>float</code> <p>Activated parameter count in billions (equals total for dense).</p> <code>weight_precision</code> <code>str</code> <p>Weight precision (e.g. <code>\"bfloat16\"</code>, <code>\"fp8\"</code>).</p> <code>gpu_model</code> <code>str</code> <p>GPU model identifier (e.g. <code>\"H100\"</code>, <code>\"B200\"</code>).</p> <code>num_gpus</code> <code>int</code> <p>Number of GPUs used.</p> <code>max_num_seqs</code> <code>int</code> <p>Maximum concurrent sequences (batch size).</p> <code>seed</code> <code>int | None</code> <p>Random seed used for the benchmark run.</p> <code>num_request_repeats</code> <code>int | None</code> <p>Number of request repetitions.</p> <code>tensor_parallel</code> <code>int</code> <p>Tensor parallelism degree.</p> <code>expert_parallel</code> <code>int</code> <p>Expert parallelism degree.</p> <code>data_parallel</code> <code>int</code> <p>Data parallelism degree.</p> <code>steady_state_energy_joules</code> <code>float</code> <p>Total GPU energy during steady state in joules.</p> <code>steady_state_duration_seconds</code> <code>float</code> <p>Duration of steady state in seconds.</p> <code>energy_per_token_joules</code> <code>float</code> <p>Steady-state energy per output token in joules.</p> <code>energy_per_request_joules</code> <code>float | None</code> <p>Estimated energy per request in joules.</p> <code>output_throughput_tokens_per_sec</code> <code>float</code> <p>Steady-state output throughput in tokens/second.</p> <code>request_throughput_req_per_sec</code> <code>float | None</code> <p>Steady-state request throughput in requests/second.</p> <code>avg_power_watts</code> <code>float</code> <p>Average GPU power during steady state in watts.</p> <code>total_output_tokens</code> <code>float | None</code> <p>Total output tokens generated (over full benchmark).</p> <code>completed_requests</code> <code>float | None</code> <p>Number of completed requests (over full benchmark).</p> <code>avg_output_len</code> <code>float | None</code> <p>Average output length in tokens.</p> <code>mean_itl_ms</code> <code>float</code> <p>Mean inter-token latency in milliseconds.</p> <code>median_itl_ms</code> <code>float</code> <p>Median inter-token latency in milliseconds.</p> <code>p50_itl_ms</code> <code>float</code> <p>50th percentile inter-token latency in milliseconds.</p> <code>p90_itl_ms</code> <code>float</code> <p>90th percentile inter-token latency in milliseconds.</p> <code>p95_itl_ms</code> <code>float</code> <p>95th percentile inter-token latency in milliseconds.</p> <code>p99_itl_ms</code> <code>float</code> <p>99th percentile inter-token latency in milliseconds.</p> <code>avg_batch_size</code> <code>float | None</code> <p>Average concurrent sequences during steady state (from Prometheus).</p> <code>is_stable</code> <code>bool</code> <p>Whether this run passed stability checks.</p> <code>unstable_reason</code> <code>str</code> <p>Reason for instability (empty if stable).</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRun.read_results_json","title":"<code>read_results_json()</code>","text":"<p>Download (if needed) and return parsed results.json.</p> <p>Caches the parsed dict per instance so repeated calls are free.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRun.read_prometheus_json","title":"<code>read_prometheus_json()</code>","text":"<p>Download (if needed) and return parsed prometheus.json.</p> <p>Caches the parsed dict per instance so repeated calls are free.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRun.output_lengths","title":"<code>output_lengths(*, include_unsuccessful=False)</code>","text":"<p>Return per-request output token lengths from results.json.</p> <p>Parameters:</p> Name Type Description Default <code>include_unsuccessful</code> <code>bool</code> <p>If True, include requests that failed during benchmarking. Defaults to False.</p> <code>False</code>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRun.inter_token_latencies","title":"<code>inter_token_latencies()</code>","text":"<p>Return inter-token latency samples in seconds from results.json.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRun.timelines","title":"<code>timelines(*, metric='power.device_instant')</code>","text":"<p>Return power/temperature timeseries for this run.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Literal['power.device_instant', 'power.device_average', 'temperature']</code> <p>Which timeline to extract. Supported values: <code>\"power.device_instant\"</code>, <code>\"power.device_average\"</code>, <code>\"temperature\"</code>.</p> <code>'power.device_instant'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: timestamp, relative_time_s, value, metric.</p>"},{"location":"api/records/#llmruns","title":"LLMRuns","text":""},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns","title":"<code>mlenergy_data.records.runs.LLMRuns</code>","text":"<p>Immutable collection of LLM benchmark runs with fluent filtering.</p> <p>Iterate to get individual <code>LLMRun</code> objects:</p> <pre><code>for r in runs.task(\"gpqa\"):\n    print(r.energy_per_token_joules, r.nickname)\n\nbest = min(runs.task(\"gpqa\"), key=lambda r: r.energy_per_token_joules)\n</code></pre> <p>Example:</p> <pre><code>runs = LLMRuns.from_directory(\"/path/to/compiled/data\")\nbest = min(runs.stable().task(\"gpqa\"), key=lambda r: r.energy_per_token_joules)\nenergies = [r.energy_per_token_joules for r in runs.task(\"gpqa\")]\n</code></pre>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.download_raw_files","title":"<code>download_raw_files(file=None)</code>","text":"<p>Download all raw files for this collection in parallel.</p> <p>Downloads results.json and prometheus.json for every run in the collection. Only useful when loaded from HF Hub; no-op for local sources.</p> <p>The full unfiltered dataset is ~100 GB. Filter first to limit download size:</p> <pre><code>runs = LLMRuns.from_hf().task(\"gpqa\").download_raw_files()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Literal['results', 'prometheus'] | None</code> <p>If specified, only download this file type. Otherwise, download both <code>results.json</code> and <code>prometheus.json</code>.</p> <code>None</code>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.from_directory","title":"<code>from_directory(root, *, stable_only=True)</code>  <code>classmethod</code>","text":"<p>Load runs from a compiled data directory (parquet-first).</p> <p>Reads <code>runs/llm.parquet</code> from the compiled data repo. No raw file parsing or stability re-computation is performed.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str | Path</code> <p>Compiled data directory containing <code>runs/llm.parquet</code>.</p> required <code>stable_only</code> <code>bool</code> <p>If True (default), only return stable runs.</p> <code>True</code>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.from_hf","title":"<code>from_hf(repo_id='ml-energy/benchmark-v3', *, revision=None, stable_only=True)</code>  <code>classmethod</code>","text":"<p>Load LLM runs from a Hugging Face dataset repository.</p> <p>The default dataset is gated. Before calling this method:</p> <ol> <li>Visit https://huggingface.co/datasets/ml-energy/benchmark-v3    and request access (granted automatically).</li> <li>Set the <code>HF_TOKEN</code> environment variable to a Hugging Face    access token.</li> </ol> <p>Downloads only the parquet summary file (~few MB). Methods that need raw data (output_lengths(), timelines(), inter_token_latencies()) will automatically download the required files on first access.</p> <p>Respects the <code>HF_HOME</code> environment variable for cache location.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>HF dataset repository ID.</p> <code>'ml-energy/benchmark-v3'</code> <code>revision</code> <code>str | None</code> <p>Git revision (branch, tag, or commit hash).</p> <code>None</code> <code>stable_only</code> <code>bool</code> <p>If True (default), only return stable runs. See from_raw_results for the definition of stability.</p> <code>True</code>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.from_parquet","title":"<code>from_parquet(path, *, base_dir=None, stable_only=True)</code>  <code>classmethod</code>","text":"<p>Construct LLMRuns from a pre-built parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the parquet file.</p> required <code>base_dir</code> <code>Path | None</code> <p>If provided, resolve relative path fields against this directory.</p> <code>None</code> <code>stable_only</code> <code>bool</code> <p>If True (default), only return stable runs.</p> <code>True</code>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.from_raw_results","title":"<code>from_raw_results(*roots, tasks=None, config_dir=None, stable_only=True, n_workers=None)</code>  <code>classmethod</code>","text":"<p>Load runs from raw benchmark result directories.</p> <p>Parses <code>results.json</code> files, computes stability, and returns the filtered collection.</p> <p>A run is considered unstable if any of the following hold:</p> <ul> <li>The steady-state duration is shorter than 20 seconds.</li> <li>The energy-per-token value is missing or non-positive.</li> <li>The average batch utilization during steady state is below 85%   of the configured <code>max_num_seqs</code>.</li> <li>Cascade rule: if any batch size for a (model, task, GPU,   num_gpus) group is unstable, all larger batch sizes in the same   group are also marked unstable.</li> </ul> <p>Stability is computed jointly across all roots so the cascade rule works cross-root.</p> <p>Parameters:</p> Name Type Description Default <code>roots</code> <code>str | Path</code> <p>One or more benchmark root directories (or results sub-dirs).</p> <code>()</code> <code>tasks</code> <code>set[str] | None</code> <p>If given, only load these tasks.</p> <code>None</code> <code>config_dir</code> <code>str | Path | None</code> <p>Path to LLM config directory (model_info.json, etc.).</p> <code>None</code> <code>stable_only</code> <code>bool</code> <p>If True (default), only return stable runs. Pass False to include all runs; each run's <code>is_stable</code> and <code>unstable_reason</code> fields indicate its status.</p> <code>True</code> <code>n_workers</code> <code>int | None</code> <p>Number of parallel workers (default: auto).</p> <code>None</code>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.task","title":"<code>task(*tasks)</code>","text":"<p>Filter to runs matching any of the given tasks.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.model_id","title":"<code>model_id(*model_ids)</code>","text":"<p>Filter to runs matching any of the given model IDs.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.gpu_model","title":"<code>gpu_model(*gpu_models)</code>","text":"<p>Filter to runs matching any of the given GPU models.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.num_gpus","title":"<code>num_gpus(*counts, min=None, max=None)</code>","text":"<p>Filter to runs matching given GPU counts or a range.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>int</code> <p>Exact GPU counts to include.</p> <code>()</code> <code>min</code> <code>int | None</code> <p>Minimum GPU count (inclusive).</p> <code>None</code> <code>max</code> <code>int | None</code> <p>Maximum GPU count (inclusive).</p> <code>None</code>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.max_num_seqs","title":"<code>max_num_seqs(*sizes, min=None, max=None)</code>","text":"<p>Filter to runs matching given max_num_seqs values or a range.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>int</code> <p>Exact values to include.</p> <code>()</code> <code>min</code> <code>int | None</code> <p>Minimum value (inclusive).</p> <code>None</code> <code>max</code> <code>int | None</code> <p>Maximum value (inclusive).</p> <code>None</code>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.precision","title":"<code>precision(*prec)</code>","text":"<p>Filter to runs matching any of the given weight precisions.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.architecture","title":"<code>architecture(*arch)</code>","text":"<p>Filter to runs matching any of the given architectures.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.nickname","title":"<code>nickname(*nicknames)</code>","text":"<p>Filter to runs matching any of the given nicknames.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.stable","title":"<code>stable()</code>","text":"<p>Filter to stable runs only.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.unstable","title":"<code>unstable()</code>","text":"<p>Filter to unstable runs only.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If this collection was loaded with <code>stable_only=True</code>, since unstable runs were already filtered out at load time.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.where","title":"<code>where(predicate)</code>","text":"<p>Filter runs by an arbitrary predicate.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>Callable[[LLMRun], bool]</code> <p>Function that takes an <code>LLMRun</code> and returns True to keep it.</p> required"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.group_by","title":"<code>group_by(*fields)</code>","text":"<p>Group runs by one or more fields.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>str</code> <p>One or more <code>LLMRun</code> field names to group by.</p> <code>()</code> <p>Returns:</p> Type Description <code>dict[Any, LLMRuns]</code> <p>Single field: <code>{value: LLMRuns, ...}</code>.</p> <code>dict[Any, LLMRuns]</code> <p>Multiple fields: <code>{(v1, v2, ...): LLMRuns, ...}</code>.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert to DataFrame with one row per run.</p> <p>Private fields (path fields, HF metadata) are excluded.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.output_lengths","title":"<code>output_lengths(*, include_unsuccessful=False)</code>","text":"<p>Extract per-request output lengths.</p> <p>Calls <code>LLMRun.output_lengths()</code> on each record, which handles downloading from HF Hub if needed.</p> <p>Parameters:</p> Name Type Description Default <code>include_unsuccessful</code> <code>bool</code> <p>If True, include requests that failed during benchmarking (<code>success=False</code> in <code>results.json</code>). Defaults to False (only successful requests).</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: task, model_id, num_gpus,</p> <code>DataFrame</code> <p>max_num_seqs, output_len</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If raw results files are not available locally and the collection was not loaded from HF Hub.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.inter_token_latencies","title":"<code>inter_token_latencies()</code>","text":"<p>Extract per-token inter-token latency samples.</p> <p>Calls <code>LLMRun.inter_token_latencies()</code> on each record, which handles downloading from HF Hub if needed. Chunked-prefill artifacts (zero-valued ITL entries) are smoothed by spreading the accumulated latency across the covered tokens.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: task, model_id, num_gpus,</p> <code>DataFrame</code> <p>max_num_seqs, itl_s</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If raw results files are not available locally and the collection was not loaded from HF Hub.</p>"},{"location":"api/records/#mlenergy_data.records.runs.LLMRuns.timelines","title":"<code>timelines(*, metric='power.device_instant')</code>","text":"<p>Extract power/temperature timeseries.</p> <p>Calls <code>LLMRun.timelines()</code> on each record, which handles downloading from HF Hub if needed.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Literal['power.device_instant', 'power.device_average', 'temperature']</code> <p>Which timeline to extract. Supported values: <code>\"power.device_instant\"</code>, <code>\"power.device_average\"</code>, <code>\"temperature\"</code>.</p> <code>'power.device_instant'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: task, model_id, num_gpus,</p> <code>DataFrame</code> <p>max_num_seqs, timestamp, relative_time_s, value, metric</p>"},{"location":"api/records/#diffusionrun","title":"DiffusionRun","text":""},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRun","title":"<code>mlenergy_data.records.runs.DiffusionRun</code>  <code>dataclass</code>","text":"<p>A single diffusion model benchmark run.</p> <p>Attributes:</p> Name Type Description <code>domain</code> <code>str</code> <p>Always <code>\"diffusion\"</code>.</p> <code>task</code> <code>str</code> <p>Benchmark task (<code>\"text-to-image\"</code> or <code>\"text-to-video\"</code>).</p> <code>model_id</code> <code>str</code> <p>Full HF model identifier.</p> <code>nickname</code> <code>str</code> <p>Human-friendly display name from <code>model_info.json</code>.</p> <code>total_params_billions</code> <code>float</code> <p>Total parameter count in billions.</p> <code>activated_params_billions</code> <code>float</code> <p>Activated parameter count in billions.</p> <code>weight_precision</code> <code>str</code> <p>Weight precision (e.g. <code>\"bfloat16\"</code>, <code>\"fp8\"</code>).</p> <code>gpu_model</code> <code>str</code> <p>GPU model identifier (e.g. <code>\"H100\"</code>).</p> <code>num_gpus</code> <code>int</code> <p>Number of GPUs used.</p> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>inference_steps</code> <code>int | None</code> <p>Number of diffusion inference steps.</p> <code>height</code> <code>int</code> <p>Output height in pixels.</p> <code>width</code> <code>int</code> <p>Output width in pixels.</p> <code>num_frames</code> <code>int | None</code> <p>Number of video frames (<code>None</code> for images).</p> <code>fps</code> <code>int | None</code> <p>Video frames per second (<code>None</code> for images).</p> <code>ulysses_degree</code> <code>int | None</code> <p>Ulysses sequence parallelism degree.</p> <code>ring_degree</code> <code>int | None</code> <p>Ring attention parallelism degree.</p> <code>use_torch_compile</code> <code>bool | None</code> <p>Whether torch.compile was enabled.</p> <code>batch_latency_s</code> <code>float</code> <p>Average batch latency in seconds.</p> <code>avg_power_watts</code> <code>float</code> <p>Average GPU power in watts.</p> <code>energy_per_generation_joules</code> <code>float</code> <p>Energy per generated output (image or video) in joules.</p> <code>throughput_generations_per_sec</code> <code>float</code> <p>Throughput in generations per second.</p>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRun.is_text_to_image","title":"<code>is_text_to_image</code>  <code>property</code>","text":"<p>Whether this is a text-to-image run.</p>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRun.is_text_to_video","title":"<code>is_text_to_video</code>  <code>property</code>","text":"<p>Whether this is a text-to-video run.</p>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRun.read_results_json","title":"<code>read_results_json()</code>","text":"<p>Download (if needed) and return parsed results.json.</p> <p>Caches the parsed dict per instance so repeated calls are free.</p>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRun.timelines","title":"<code>timelines(*, metric='power.device_instant')</code>","text":"<p>Return power/temperature timeseries for this run.</p> <p>Diffusion runs do not have steady-state bounds, so the full timeline is returned.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Literal['power.device_instant', 'power.device_average', 'temperature']</code> <p>Which timeline to extract. Supported values: <code>\"power.device_instant\"</code>, <code>\"power.device_average\"</code>, <code>\"temperature\"</code>.</p> <code>'power.device_instant'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: timestamp, relative_time_s, value, metric.</p>"},{"location":"api/records/#diffusionruns","title":"DiffusionRuns","text":""},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns","title":"<code>mlenergy_data.records.runs.DiffusionRuns</code>","text":"<p>Immutable collection of diffusion model benchmark runs with fluent filtering.</p> <p>Same collection pattern as <code>LLMRuns</code>, with diffusion-specific filters.</p> <p>Iterate to get individual <code>DiffusionRun</code> objects:</p> <pre><code>for r in runs.task(\"text-to-image\"):\n    print(r.energy_per_generation_joules, r.nickname)\n</code></pre> <p>Example:</p> <pre><code>runs = DiffusionRuns.from_directory(\"/path/to/compiled/data\")\npowers = [r.avg_power_watts for r in runs.task(\"text-to-image\")]\n</code></pre>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.download_raw_files","title":"<code>download_raw_files()</code>","text":"<p>Download all raw files for this collection in parallel.</p> <p>Downloads results.json for every run in the collection. Only useful when loaded from HF Hub; no-op for local sources.</p> <p>The full unfiltered dataset is ~100 GB. Filter first to limit download size:</p> <pre><code>runs = DiffusionRuns.from_hf().task(\"text-to-image\").download_raw_files()\n</code></pre>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.from_directory","title":"<code>from_directory(root)</code>  <code>classmethod</code>","text":"<p>Load runs from a compiled data directory (parquet-first).</p> <p>Reads <code>runs/diffusion.parquet</code> from the compiled data repo.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str | Path</code> <p>Compiled data directory containing <code>runs/diffusion.parquet</code>.</p> required"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.from_hf","title":"<code>from_hf(repo_id='ml-energy/benchmark-v3', *, revision=None)</code>  <code>classmethod</code>","text":"<p>Load diffusion runs from a Hugging Face dataset repository.</p> <p>The default dataset is gated. Before calling this method:</p> <ol> <li>Visit https://huggingface.co/datasets/ml-energy/benchmark-v3    and request access (granted automatically).</li> <li>Set the <code>HF_TOKEN</code> environment variable to a Hugging Face    access token.</li> </ol> <p>Downloads only the parquet summary file (~few MB). Methods that need raw data (timelines()) will automatically download the required files on first access.</p> <p>Respects the <code>HF_HOME</code> environment variable for cache location.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>HF dataset repository ID.</p> <code>'ml-energy/benchmark-v3'</code> <code>revision</code> <code>str | None</code> <p>Git revision (branch, tag, or commit hash).</p> <code>None</code>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.from_parquet","title":"<code>from_parquet(path, *, base_dir=None)</code>  <code>classmethod</code>","text":"<p>Construct DiffusionRuns from a pre-built parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the parquet file.</p> required <code>base_dir</code> <code>Path | None</code> <p>If provided, resolve relative path fields against this directory.</p> <code>None</code>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.from_raw_results","title":"<code>from_raw_results(*roots, tasks=None, config_dir=None, n_workers=None)</code>  <code>classmethod</code>","text":"<p>Load runs from raw benchmark result directories.</p> <p>Parses <code>results.json</code> files and returns the collection.</p> <p>Parameters:</p> Name Type Description Default <code>roots</code> <code>str | Path</code> <p>One or more benchmark root directories (or results sub-dirs).</p> <code>()</code> <code>tasks</code> <code>set[str] | None</code> <p>If given, only load these tasks.</p> <code>None</code> <code>config_dir</code> <code>str | Path | None</code> <p>Path to diffusion config directory.</p> <code>None</code> <code>n_workers</code> <code>int | None</code> <p>Number of parallel workers (default: auto).</p> <code>None</code>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.task","title":"<code>task(*tasks)</code>","text":"<p>Filter to runs matching any of the given tasks.</p>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.model_id","title":"<code>model_id(*model_ids)</code>","text":"<p>Filter to runs matching any of the given model IDs.</p>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.gpu_model","title":"<code>gpu_model(*gpu_models)</code>","text":"<p>Filter to runs matching any of the given GPU models.</p>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.num_gpus","title":"<code>num_gpus(*counts, min=None, max=None)</code>","text":"<p>Filter to runs matching given GPU counts or a range.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>int</code> <p>Exact GPU counts to include.</p> <code>()</code> <code>min</code> <code>int | None</code> <p>Minimum GPU count (inclusive).</p> <code>None</code> <code>max</code> <code>int | None</code> <p>Maximum GPU count (inclusive).</p> <code>None</code>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.nickname","title":"<code>nickname(*nicknames)</code>","text":"<p>Filter to runs matching any of the given nicknames.</p>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.batch_size","title":"<code>batch_size(*sizes, min=None, max=None)</code>","text":"<p>Filter to runs matching given batch sizes or a range.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>int</code> <p>Exact batch sizes to include.</p> <code>()</code> <code>min</code> <code>int | None</code> <p>Minimum batch size (inclusive).</p> <code>None</code> <code>max</code> <code>int | None</code> <p>Maximum batch size (inclusive).</p> <code>None</code>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.precision","title":"<code>precision(*prec)</code>","text":"<p>Filter to runs matching any of the given weight precisions.</p>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.where","title":"<code>where(predicate)</code>","text":"<p>Filter runs by an arbitrary predicate.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>Callable[[DiffusionRun], bool]</code> <p>Function that takes a <code>DiffusionRun</code> and returns True to keep it.</p> required"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.group_by","title":"<code>group_by(*fields)</code>","text":"<p>Group runs by one or more fields.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>str</code> <p>One or more <code>DiffusionRun</code> field names to group by.</p> <code>()</code> <p>Returns:</p> Type Description <code>dict[Any, DiffusionRuns]</code> <p>Single field: <code>{value: DiffusionRuns, ...}</code>.</p> <code>dict[Any, DiffusionRuns]</code> <p>Multiple fields: <code>{(v1, v2, ...): DiffusionRuns, ...}</code>.</p>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert to DataFrame with one row per run.</p> <p>Private fields (path fields, HF metadata) are excluded.</p>"},{"location":"api/records/#mlenergy_data.records.runs.DiffusionRuns.timelines","title":"<code>timelines(*, metric='power.device_instant')</code>","text":"<p>Extract power/temperature timeseries.</p> <p>Calls <code>DiffusionRun.timelines()</code> on each record, which handles downloading from HF Hub if needed.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Literal['power.device_instant', 'power.device_average', 'temperature']</code> <p>Which timeline to extract. Supported values: <code>\"power.device_instant\"</code>, <code>\"power.device_average\"</code>, <code>\"temperature\"</code>.</p> <code>'power.device_instant'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: task, model_id, num_gpus,</p> <code>DataFrame</code> <p>batch_size, timestamp, relative_time_s, value, metric</p>"}]}